{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pvlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikolaushouben/Library/Mobile Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaushouben/Library/Mobile%20Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative%20Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaushouben/Library/Mobile%20Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative%20Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# pvlib imports\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikolaushouben/Library/Mobile%20Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative%20Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpvlib\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaushouben/Library/Mobile%20Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative%20Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpvlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocation\u001b[39;00m \u001b[39mimport\u001b[39;00m Location\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaushouben/Library/Mobile%20Documents/com~apple~CloudDocs/PhD_Icloud/PhD_Papers/Paper_2_Collaborative%20Forecasting/Code/Collaborative_PV_Forecasting/notebooks/inverse_pv_yield_model.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpvlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpvsystem\u001b[39;00m \u001b[39mimport\u001b[39;00m PVSystem\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pvlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# pvlib imports\n",
    "import pvlib\n",
    "from pvlib.location import Location\n",
    "from pvlib.pvsystem import PVSystem\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"../../Input_Data/Meta_Data_PV_Optimised.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model clear sky for all systems\n",
    "\n",
    "df_meteo = pd.read_csv(\"../../Input_Data/ghi_dni_dhi.csv\", index_col=0, parse_dates=True)\n",
    "print(df_meteo.shape)\n",
    "print(df_meteo.columns)\n",
    "df_meteo = df_meteo.drop_duplicates() #we only need independent samples of the meteo data\n",
    "print(df_meteo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latitude = 52.0878 \n",
    "longitude =  5.16622\n",
    "location_meteo = Location(latitude=latitude, longitude=longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physical_profile(row):\n",
    "    index, latitude, longitude, tilt, azimuth, capacity = row\n",
    "\n",
    "    temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_glass']\n",
    "\n",
    "    location = Location(latitude=latitude, longitude=longitude)\n",
    "\n",
    "    pvwatts_system = PVSystem(surface_tilt= tilt , surface_azimuth=azimuth,\n",
    "        module_parameters={'pdc0': capacity, 'gamma_pdc': -0.004},\n",
    "        inverter_parameters={'pdc0': capacity},\n",
    "        temperature_model_parameters=temperature_model_parameters)\n",
    "    \n",
    "    mc = ModelChain(pvwatts_system, location, aoi_model='physical', spectral_model='no_loss')\n",
    "    mc.run_model(df_meteo)\n",
    "    results = mc.results.ac\n",
    "\n",
    "    df_results = pd.Series(results)\n",
    "    df_results.index = df_results.index.tz_localize(None)\n",
    "    df_results.index.name = \"timestamp\"\n",
    "\n",
    "    return df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "tilt_azimuth_combinations = list(itertools.product(range(0,91,1), range(0,361,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilt_azi_np_array = np.array(tilt_azimuth_combinations)\n",
    "\n",
    "tilt_azi_np_array = df_meta.values\n",
    "\n",
    "tilt_azi_np_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_con = pd.DataFrame({\"lat\": latitude, \"lon\": longitude, \"tilt\":0, \"azimuth\": 0, \"capacity\":1}, index = range(tilt_azi_np_array.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_con[[\"tilt\", \"azimuth\"]] = tilt_azi_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_con = df_meta_con.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical = df_meta_con.apply(physical_profile, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical_T = df_physical.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arrays = []\n",
    "\n",
    "for col in df_physical_T:\n",
    "    df_col = df_physical_T[[col]]\n",
    "    df_col[\"tilt\"] = df_meta_con.iloc[col,:][\"tilt\"]\n",
    "    df_col[\"azimuth\"] = df_meta_con.iloc[col,:][\"azimuth\"]\n",
    "    df_col[\"dhi\"] = df_meteo[\"dhi\"]\n",
    "    df_col[\"dni\"] = df_meteo[\"dni\"]\n",
    "    df_col[\"ghi\"] = df_meteo[\"ghi\"]\n",
    "    array = df_col.to_numpy()\n",
    "    arrays.append(array)\n",
    "\n",
    "\n",
    "arrays_stacked = np.stack(arrays)\n",
    "x,y,z = arrays_stacked.shape\n",
    "arrays_stacked_reshaped = arrays_stacked.reshape(1,x*y, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = arrays_stacked_reshaped.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = pd.DataFrame(samples ,columns=[\"power\", \"tilt\", \"azimuth\", \"dhi\", \"dni\", \"ghi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples.to_parquet(\"../../Input_Data/inverse_samples.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read-In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = pd.read_parquet(\"../../Input_Data/inverse_samples.parquet\")\n",
    "scaler2 = MinMaxScaler()\n",
    "df_samples[df_samples.columns] = scaler2.fit_transform(df_samples[df_samples.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training A Neural Network with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy = df_samples.iloc[:,:3].to_numpy()\n",
    "y_numpy = df_samples.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.from_numpy(x_numpy).float()\n",
    "y_tensor = torch.from_numpy(y_numpy).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m , n = x_tensor.shape\n",
    "m\n",
    "\n",
    "train_len = round(m*0.8)\n",
    "val_len = round(m*0.2)\n",
    "\n",
    "\n",
    "assert train_len + val_len == m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_len, val_len] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer): # this is a useful closure for the train step\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3,3),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-1\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch,y_batch)\n",
    "        losses.append(loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(val_losses)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_samples.iloc[:,:3], df_samples.iloc[:,-1:], test_size = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "#the two most important parameters for XGBoost. \n",
    "param_grid = {\n",
    "                'eta':[0.2, 0.3, 0.4, 0.5], \n",
    "                'max_depth': [1,2,4,8] \n",
    "            }\n",
    "\n",
    "cv = TimeSeriesSplit(4, gap = 1)\n",
    "gs = GridSearchCV(model, param_grid, cv=cv, scoring = \"neg_root_mean_squared_error\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "\n",
    "best_model.fit(X_train,y_train)\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "test_compare = y_test.copy().iloc[:,:1]\n",
    "test_compare[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[\"predictions\"] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test.iloc[:,:1], y_test.iloc[:,1:], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e435237ca0942949f06fff12a56a8037995b0730e0492b1085cd436398fe123"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
